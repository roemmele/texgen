{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are some snapshots of the steps for training and generating from a model using the toy datasets in test/toy_data/. \n",
    "Note: To demonstrate how a model learns to fit a particular training set, the same data (the files prefixed with \"train\") is used for both validation and generation. To evaluate a model in a \"real\" experimental setting, it should instead be applied to held-out data (e.g. the files prefixed with \"eval\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define data and model files'''\n",
    "\n",
    "# Infilling toy data\n",
    "train_src_file = \"test/toy_data/infilling/train.src\"\n",
    "train_tgt_file = \"test/toy_data/infilling/train.tgt\"\n",
    "model_config_file = \"test/test_configs/gpt2_lm_config.json\"\n",
    "save_model_dir = \"infilling_test_model\"\n",
    "gen_output_file = \"test/toy_data/infilling/train.gen\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:         Harry Potter Philosopher's Stone\n",
      "TARGET:         Harry Potter and the Philosopher's Stone\n",
      "\n",
      "SOURCE:         Harry Potter and Secrets\n",
      "TARGET:         Harry Potter and the Chamber of Secrets\n",
      "\n",
      "SOURCE:         Harry the Prisoner of Azkaban\n",
      "TARGET:         Harry Potter and the Prisoner of Azkaban\n",
      "\n",
      "SOURCE:         Potter and the Goblet of\n",
      "TARGET:         Harry Potter and the Goblet of Fire\n",
      "\n",
      "SOURCE:         Harry Potter the Order the Phoenix\n",
      "TARGET:         Harry Potter and the Order of the Phoenix\n",
      "\n",
      "SOURCE:         and the Half-Blood Prince\n",
      "TARGET:         Harry Potter and the Half-Blood Prince\n",
      "\n",
      "SOURCE:         Harry Hallows\n",
      "TARGET:         Harry Potter and the Deathly Hallows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Show format of source-target pairs used for training'''\n",
    "\n",
    "with open(train_src_file) as src_f,\\\n",
    "    open(train_tgt_file) as tgt_f:\n",
    "    src_texts = [text.strip() for text in src_f]\n",
    "    tgt_texts = [text.strip() for text in tgt_f]\n",
    "\n",
    "for src, tgt in zip(src_texts, tgt_texts):\n",
    "    print(\"{:15s} {}\".format(\"SOURCE:\", src))\n",
    "    print(\"{:15s} {}\\n\".format(\"TARGET:\", tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize early training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_dir': 'infilling_test_model', 'config_file': 'test/test_configs/gpt2_lm_config.json', 'train_src_file': 'test/toy_data/infilling/train.src', 'train_tgt_file': 'test/toy_data/infilling/train.tgt', 'eval_src_file': 'test/toy_data/infilling/train.src', 'eval_tgt_file': 'test/toy_data/infilling/train.tgt', 'train_ref_file': None, 'eval_ref_file': None, 'max_src_length': 25, 'max_tgt_length': 75, 'load_from_dir': None, 'pg_metrics': [], 'eval_metrics': [], 'batch_size': 32, 'max_epochs': 100, 'learning_rate': 0.001, 'patience': 5, 'dynamic_lr': False, 'warmup_steps': 4000, 'max_grad_norm': 5.0, 'accum_steps': 1, 'log_iterations': 100, 'valid_iterations': 1000, 'valid_epoch_end': True}\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.construct:Saved model configuration to infilling_test_model/hparams.json\n",
      "INFO:texgen.construct:Created LM model\n",
      "INFO:texgen.construct:# trainable parameters in model: 152806656\n",
      "INFO:texgen.train:Validation result prior to training:\n",
      "2021-11-16 16:03:51 : Epoch 0, test result = {Average: 97.714}\n",
      "\u001b[31mWARNING 2021-11-16 16:03:51 : Specified checkpoint directory 'infilling_test_model' exists, previous checkpoints might be erased\u001b[0m\n",
      "\u001b[32mINFO 2021-11-16 16:03:51 : \u001b[0mTraining started\n",
      "\u001b[32mINFO 2021-11-16 16:03:51 : \u001b[0mModel architecture:\n",
      "LM(\n",
      "  (decoder): TransformerLMWrapper(\n",
      "    (word_embedder): WordEmbedder(\n",
      "      vocab_size=50257, embedding_dim=768\n",
      "      (_dropout_layer): EmbeddingDropout()\n",
      "    )\n",
      "    (position_embedder): PositionEmbedder(\n",
      "      position_size=1024, embedding_dim=768\n",
      "      (_dropout_layer): EmbeddingDropout()\n",
      "    )\n",
      "    (decoder): TransformerLM(\n",
      "      (_output_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      (self_attns): ModuleList(\n",
      "        (ids 0-11): MultiheadAttentionEncoder(\n",
      "          (Q_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (K_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (V_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (O_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (self_attn_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (enc_dec_attns): ModuleList(\n",
      "        (ids 0-11): MultiheadAttentionEncoder(\n",
      "          (Q_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (K_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (V_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (O_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (end_dec_attn_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (poswise_networks): ModuleList(\n",
      "        (ids 0-11): FeedForwardNetwork(\n",
      "          (_layers): ModuleList(\n",
      "            (id 0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (id 1): GPTGELU()\n",
      "            (id 2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (poswise_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (residual_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2021-11-16 16:03:56 : Epoch 1, valid result = {Average: 69.029}\n",
      "\u001b[32mINFO 2021-11-16 16:03:57 : \u001b[0mPrevious checkpoint 1637107339.168591.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:04:00 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637107437.0319.pt\n",
      "2021-11-16 16:04:10 : Epoch 2, valid result = {Average: 139.793}\n",
      "\u001b[32mINFO 2021-11-16 16:04:10 : \u001b[0mEarly stopping patience decrease to 4\n",
      "2021-11-16 16:04:15 : Epoch 3, valid result = {Average: 64.710}\n",
      "\u001b[32mINFO 2021-11-16 16:04:15 : \u001b[0mPrevious checkpoint 1637107437.0319.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:04:18 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637107455.18626.pt\n",
      "2021-11-16 16:04:23 : Epoch 4, valid result = {Average: 39.352}\n",
      "\u001b[32mINFO 2021-11-16 16:04:23 : \u001b[0mPrevious checkpoint 1637107455.18626.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:04:25 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637107463.672299.pt\n",
      "2021-11-16 16:04:30 : Epoch 5, valid result = {Average: 41.754}\n",
      "\u001b[32mINFO 2021-11-16 16:04:30 : \u001b[0mEarly stopping patience decrease to 3\n",
      "2021-11-16 16:04:35 : Epoch 6, valid result = {Average: 42.768}\n",
      "\u001b[32mINFO 2021-11-16 16:04:35 : \u001b[0mEarly stopping patience decrease to 2\n",
      "2021-11-16 16:04:39 : Epoch 7, valid result = {Average: 37.332}\n",
      "\u001b[32mINFO 2021-11-16 16:04:39 : \u001b[0mPrevious checkpoint 1637107463.672299.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:04:42 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637107479.144083.pt\n",
      "2021-11-16 16:04:47 : Epoch 8, valid result = {Average: 33.570}\n",
      "\u001b[32mINFO 2021-11-16 16:04:47 : \u001b[0mPrevious checkpoint 1637107479.144083.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:04:50 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637107487.881969.pt\n",
      "2021-11-16 16:04:56 : Epoch 9, valid result = {Average: 34.049}\n",
      "\u001b[32mINFO 2021-11-16 16:04:56 : \u001b[0mEarly stopping patience decrease to 1\n",
      "2021-11-16 16:05:02 : Epoch 10, valid result = {Average: 31.860}\n",
      "\u001b[32mINFO 2021-11-16 16:05:02 : \u001b[0mPrevious checkpoint 1637107487.881969.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:05:05 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637107502.633285.pt\n",
      "2021-11-16 16:05:10 : Epoch 11, valid result = {Average: 30.193}\n",
      "\u001b[32mINFO 2021-11-16 16:05:10 : \u001b[0mPrevious checkpoint 1637107502.633285.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:05:13 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637107510.10613.pt\n",
      "2021-11-16 16:05:19 : Epoch 12, valid result = {Average: 30.944}\n",
      "\u001b[32mINFO 2021-11-16 16:05:19 : \u001b[0mEarly stopping patience exhausted, terminating training...\n",
      "\u001b[32mINFO 2021-11-16 16:05:19 : \u001b[0mTraining terminated\n"
     ]
    }
   ],
   "source": [
    "'''Train a model to predict target texts from source inputs'''\n",
    "\n",
    "!python train_script.py\\\n",
    "    -train_src_file {train_src_file}\\\n",
    "    -train_tgt_file {train_tgt_file}\\\n",
    "    -eval_src_file {train_src_file}\\\n",
    "    -eval_tgt_file {train_tgt_file}\\\n",
    "    -config_file {model_config_file}\\\n",
    "    -save_dir {save_model_dir}\\\n",
    "    -patience 5\\\n",
    "    -valid_epoch_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_dir': 'infilling_test_model', 'gen_texts_file': 'test/toy_data/infilling/train.gen', 'src_texts_file': 'test/toy_data/infilling/train.src', 'max_decoding_length': 100, 'n_gen_per_src': 1, 'batch_size': 64, 'min_postproc_length': None, 'max_postproc_length': None, 'max_redundancy_rate': None, 'block_repeat': False, 'block_quotes': False, 'block_profanity': False, 'require_paired_punct': False, 'require_eos_punct': False, 'require_src_in_gen': False, 'force_src_in_regen': False, 'max_gen_attempts': 1, 'fallback_to_src': False, 'infer_method': 'sample', 'sample_top_k': 0, 'sample_p': 0.7, 'sample_temperature': 1.0, 'verbose': True}\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.construct:Loaded LM model from infilling_test_model/1637107510.10613.pt\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.generate:Starting generation round 1, attempt 1...\n",
      "INFO:texgen.generate:round 1, attempt 1: generated 7 texts (0 failed requirements)...\n",
      "INFO:texgen.generate:Saved generated texts to test/toy_data/infilling/train.gen\n"
     ]
    }
   ],
   "source": [
    "'''Generate texts for source inputs'''\n",
    "\n",
    "!python generation_script.py\\\n",
    "    -src_texts_file {train_src_file}\\\n",
    "    -model_dir {save_model_dir}\\\n",
    "    -gen_texts_file {gen_output_file}\\\n",
    "    -infer_method sample\\\n",
    "    -sample_p 0.7\\\n",
    "    -verbose\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:         Harry Potter Philosopher's Stone\n",
      "GENERATED:      and of the and Potter\n",
      "\n",
      "SOURCE:         Harry Potter and Secrets\n",
      "GENERATED:      and Potter the the theHarry and theHarry's and the of the\n",
      "\n",
      "SOURCE:         Harry the Prisoner of Azkaban\n",
      "GENERATED:      PotterHarry\n",
      "\n",
      "SOURCE:         Potter and the Goblet of\n",
      "GENERATED:      the PotterHarryHarry the the the\n",
      "\n",
      "SOURCE:         Harry Potter the Order the Phoenix\n",
      "GENERATED:      of the and the of Potter and the of of and and of and of and PotterHarry the\n",
      "\n",
      "SOURCE:         and the Half-Blood Prince\n",
      "GENERATED:      \n",
      "\n",
      "SOURCE:         Harry Hallows\n",
      "GENERATED:      Potter the theHarry and and of and and the and of PotterHarryHarry of the of and and the and of the of the PotterHarry the the theHarry the and the and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''View generated texts'''\n",
    "\n",
    "with open(train_src_file) as src_f,\\\n",
    "    open(gen_output_file) as gen_f:\n",
    "    src_texts = [text.strip() for text in src_f]\n",
    "    gen_texts = [text.strip() for text in gen_f]\n",
    "\n",
    "for src, gen in zip(src_texts, gen_texts):\n",
    "    print(\"{:15s} {}\".format(\"SOURCE:\", src))\n",
    "    print(\"{:15s} {}\\n\".format(\"GENERATED:\", gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue training to improve generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_dir': 'infilling_test_model', 'config_file': 'test/test_configs/gpt2_lm_config.json', 'train_src_file': 'test/toy_data/infilling/train.src', 'train_tgt_file': 'test/toy_data/infilling/train.tgt', 'eval_src_file': 'test/toy_data/infilling/train.src', 'eval_tgt_file': 'test/toy_data/infilling/train.tgt', 'train_ref_file': None, 'eval_ref_file': None, 'max_src_length': 25, 'max_tgt_length': 75, 'load_from_dir': 'infilling_test_model', 'pg_metrics': [], 'eval_metrics': [], 'batch_size': 32, 'max_epochs': 100, 'learning_rate': 0.001, 'patience': 50, 'dynamic_lr': False, 'warmup_steps': 4000, 'max_grad_norm': 5.0, 'accum_steps': 1, 'log_iterations': 100, 'valid_iterations': 1000, 'valid_epoch_end': True}\n",
      "INFO:texgen.train:Loading model configuration from infilling_test_model. All hyperparameter settings will be read from here and will override any settings provided as command-line arguments.\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.construct:Saved model configuration to infilling_test_model/hparams.json\n",
      "INFO:texgen.construct:Created LM model\n",
      "INFO:texgen.construct:# trainable parameters in model: 152806656\n",
      "INFO:texgen.train:Initialized model from checkpoint infilling_test_model/1637107510.10613.pt\n",
      "INFO:texgen.train:Validation result prior to training:\n",
      "2021-11-16 16:17:21 : Epoch 0, test result = {Average: 30.193}\n",
      "\u001b[31mWARNING 2021-11-16 16:17:21 : Specified checkpoint directory 'infilling_test_model' exists, previous checkpoints might be erased\u001b[0m\n",
      "\u001b[32mINFO 2021-11-16 16:17:21 : \u001b[0mTraining started\n",
      "\u001b[32mINFO 2021-11-16 16:17:21 : \u001b[0mModel architecture:\n",
      "LM(\n",
      "  (decoder): TransformerLMWrapper(\n",
      "    (word_embedder): WordEmbedder(\n",
      "      vocab_size=50257, embedding_dim=768\n",
      "      (_dropout_layer): EmbeddingDropout()\n",
      "    )\n",
      "    (position_embedder): PositionEmbedder(\n",
      "      position_size=1024, embedding_dim=768\n",
      "      (_dropout_layer): EmbeddingDropout()\n",
      "    )\n",
      "    (decoder): TransformerLM(\n",
      "      (_output_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      (self_attns): ModuleList(\n",
      "        (ids 0-11): MultiheadAttentionEncoder(\n",
      "          (Q_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (K_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (V_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (O_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (self_attn_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (enc_dec_attns): ModuleList(\n",
      "        (ids 0-11): MultiheadAttentionEncoder(\n",
      "          (Q_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (K_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (V_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (O_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (end_dec_attn_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (poswise_networks): ModuleList(\n",
      "        (ids 0-11): FeedForwardNetwork(\n",
      "          (_layers): ModuleList(\n",
      "            (id 0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (id 1): GPTGELU()\n",
      "            (id 2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (poswise_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (residual_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2021-11-16 16:17:26 : Epoch 1, valid result = {Average: 30.944}\n",
      "\u001b[32mINFO 2021-11-16 16:17:26 : \u001b[0mPrevious checkpoint 1637107510.10613.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:17:28 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108246.381622.pt\n",
      "2021-11-16 16:17:33 : Epoch 2, valid result = {Average: 31.028}\n",
      "\u001b[32mINFO 2021-11-16 16:17:33 : \u001b[0mEarly stopping patience decrease to 49\n",
      "2021-11-16 16:17:38 : Epoch 3, valid result = {Average: 28.778}\n",
      "\u001b[32mINFO 2021-11-16 16:17:38 : \u001b[0mPrevious checkpoint 1637108246.381622.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:17:41 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108258.620399.pt\n",
      "2021-11-16 16:17:46 : Epoch 4, valid result = {Average: 28.226}\n",
      "\u001b[32mINFO 2021-11-16 16:17:46 : \u001b[0mPrevious checkpoint 1637108258.620399.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:17:49 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108266.267169.pt\n",
      "2021-11-16 16:17:53 : Epoch 5, valid result = {Average: 27.917}\n",
      "\u001b[32mINFO 2021-11-16 16:17:53 : \u001b[0mPrevious checkpoint 1637108266.267169.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:17:56 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108273.513516.pt\n",
      "2021-11-16 16:18:00 : Epoch 6, valid result = {Average: 27.767}\n",
      "\u001b[32mINFO 2021-11-16 16:18:00 : \u001b[0mPrevious checkpoint 1637108273.513516.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:04 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108280.865132.pt\n",
      "2021-11-16 16:18:08 : Epoch 7, valid result = {Average: 27.654}\n",
      "\u001b[32mINFO 2021-11-16 16:18:08 : \u001b[0mPrevious checkpoint 1637108280.865132.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:11 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108288.5354738.pt\n",
      "2021-11-16 16:18:16 : Epoch 8, valid result = {Average: 27.003}\n",
      "\u001b[32mINFO 2021-11-16 16:18:16 : \u001b[0mPrevious checkpoint 1637108288.5354738.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:19 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108296.803217.pt\n",
      "2021-11-16 16:18:24 : Epoch 9, valid result = {Average: 26.880}\n",
      "\u001b[32mINFO 2021-11-16 16:18:24 : \u001b[0mPrevious checkpoint 1637108296.803217.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:27 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108304.7754161.pt\n",
      "2021-11-16 16:18:34 : Epoch 10, valid result = {Average: 26.709}\n",
      "\u001b[32mINFO 2021-11-16 16:18:34 : \u001b[0mPrevious checkpoint 1637108304.7754161.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:36 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108314.144682.pt\n",
      "2021-11-16 16:18:41 : Epoch 11, valid result = {Average: 26.062}\n",
      "\u001b[32mINFO 2021-11-16 16:18:41 : \u001b[0mPrevious checkpoint 1637108314.144682.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:43 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108321.32022.pt\n",
      "2021-11-16 16:18:48 : Epoch 12, valid result = {Average: 25.457}\n",
      "\u001b[32mINFO 2021-11-16 16:18:48 : \u001b[0mPrevious checkpoint 1637108321.32022.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:52 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108328.79354.pt\n",
      "2021-11-16 16:18:56 : Epoch 13, valid result = {Average: 24.501}\n",
      "\u001b[32mINFO 2021-11-16 16:18:57 : \u001b[0mPrevious checkpoint 1637108328.79354.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:18:59 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108337.002199.pt\n",
      "2021-11-16 16:19:04 : Epoch 14, valid result = {Average: 22.470}\n",
      "\u001b[32mINFO 2021-11-16 16:19:04 : \u001b[0mPrevious checkpoint 1637108337.002199.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:19:07 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108344.6467898.pt\n",
      "2021-11-16 16:19:12 : Epoch 15, valid result = {Average: 20.216}\n",
      "\u001b[32mINFO 2021-11-16 16:19:12 : \u001b[0mPrevious checkpoint 1637108344.6467898.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:19:15 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108352.995719.pt\n",
      "2021-11-16 16:19:20 : Epoch 16, valid result = {Average: 19.034}\n",
      "\u001b[32mINFO 2021-11-16 16:19:20 : \u001b[0mPrevious checkpoint 1637108352.995719.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:19:23 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108360.819128.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-16 16:19:28 : Epoch 17, valid result = {Average: 16.470}\n",
      "\u001b[32mINFO 2021-11-16 16:19:28 : \u001b[0mPrevious checkpoint 1637108360.819128.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:19:30 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108368.203233.pt\n",
      "2021-11-16 16:19:35 : Epoch 18, valid result = {Average: 15.319}\n",
      "\u001b[32mINFO 2021-11-16 16:19:35 : \u001b[0mPrevious checkpoint 1637108368.203233.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:19:39 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108375.242122.pt\n",
      "2021-11-16 16:19:46 : Epoch 19, valid result = {Average: 15.402}\n",
      "\u001b[32mINFO 2021-11-16 16:19:46 : \u001b[0mEarly stopping patience decrease to 48\n",
      "2021-11-16 16:19:50 : Epoch 20, valid result = {Average: 12.980}\n",
      "\u001b[32mINFO 2021-11-16 16:19:50 : \u001b[0mPrevious checkpoint 1637108375.242122.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:19:53 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108390.387085.pt\n",
      "2021-11-16 16:19:57 : Epoch 21, valid result = {Average: 12.783}\n",
      "\u001b[32mINFO 2021-11-16 16:19:57 : \u001b[0mPrevious checkpoint 1637108390.387085.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:01 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108397.884133.pt\n",
      "2021-11-16 16:20:05 : Epoch 22, valid result = {Average: 10.961}\n",
      "\u001b[32mINFO 2021-11-16 16:20:05 : \u001b[0mPrevious checkpoint 1637108397.884133.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:08 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108405.6900532.pt\n",
      "2021-11-16 16:20:12 : Epoch 23, valid result = {Average: 10.261}\n",
      "\u001b[32mINFO 2021-11-16 16:20:12 : \u001b[0mPrevious checkpoint 1637108405.6900532.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:15 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108412.7867892.pt\n",
      "2021-11-16 16:20:20 : Epoch 24, valid result = {Average: 9.815}\n",
      "\u001b[32mINFO 2021-11-16 16:20:20 : \u001b[0mPrevious checkpoint 1637108412.7867892.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:22 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108420.1361861.pt\n",
      "2021-11-16 16:20:26 : Epoch 25, valid result = {Average: 8.817}\n",
      "\u001b[32mINFO 2021-11-16 16:20:27 : \u001b[0mPrevious checkpoint 1637108420.1361861.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:30 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108427.129745.pt\n",
      "2021-11-16 16:20:34 : Epoch 26, valid result = {Average: 8.259}\n",
      "\u001b[32mINFO 2021-11-16 16:20:34 : \u001b[0mPrevious checkpoint 1637108427.129745.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:37 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108434.781188.pt\n",
      "2021-11-16 16:20:41 : Epoch 27, valid result = {Average: 6.437}\n",
      "\u001b[32mINFO 2021-11-16 16:20:41 : \u001b[0mPrevious checkpoint 1637108434.781188.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:45 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108441.919911.pt\n",
      "2021-11-16 16:20:49 : Epoch 28, valid result = {Average: 6.437}\n",
      "\u001b[32mINFO 2021-11-16 16:20:49 : \u001b[0mPrevious checkpoint 1637108441.919911.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:20:52 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108449.9880981.pt\n",
      "2021-11-16 16:21:01 : Epoch 29, valid result = {Average: 6.610}\n",
      "\u001b[32mINFO 2021-11-16 16:21:01 : \u001b[0mEarly stopping patience decrease to 47\n",
      "2021-11-16 16:21:05 : Epoch 30, valid result = {Average: 4.921}\n",
      "\u001b[32mINFO 2021-11-16 16:21:05 : \u001b[0mPrevious checkpoint 1637108449.9880981.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:21:08 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108465.714456.pt\n",
      "2021-11-16 16:21:13 : Epoch 31, valid result = {Average: 3.920}\n",
      "\u001b[32mINFO 2021-11-16 16:21:13 : \u001b[0mPrevious checkpoint 1637108465.714456.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:21:17 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108473.724983.pt\n",
      "2021-11-16 16:21:21 : Epoch 32, valid result = {Average: 3.148}\n",
      "\u001b[32mINFO 2021-11-16 16:21:21 : \u001b[0mPrevious checkpoint 1637108473.724983.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:21:24 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108481.904035.pt\n",
      "2021-11-16 16:21:29 : Epoch 33, valid result = {Average: 3.005}\n",
      "\u001b[32mINFO 2021-11-16 16:21:29 : \u001b[0mPrevious checkpoint 1637108481.904035.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:21:32 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108489.57902.pt\n",
      "2021-11-16 16:21:37 : Epoch 34, valid result = {Average: 2.179}\n",
      "\u001b[32mINFO 2021-11-16 16:21:37 : \u001b[0mPrevious checkpoint 1637108489.57902.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:21:40 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108497.58983.pt\n",
      "2021-11-16 16:21:45 : Epoch 35, valid result = {Average: 2.060}\n",
      "\u001b[32mINFO 2021-11-16 16:21:45 : \u001b[0mPrevious checkpoint 1637108497.58983.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:21:48 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108505.4342341.pt\n",
      "2021-11-16 16:21:53 : Epoch 36, valid result = {Average: 2.288}\n",
      "\u001b[32mINFO 2021-11-16 16:21:53 : \u001b[0mEarly stopping patience decrease to 46\n",
      "2021-11-16 16:21:57 : Epoch 37, valid result = {Average: 1.398}\n",
      "\u001b[32mINFO 2021-11-16 16:21:57 : \u001b[0mPrevious checkpoint 1637108505.4342341.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:22:00 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108517.875284.pt\n",
      "2021-11-16 16:22:05 : Epoch 38, valid result = {Average: 2.225}\n",
      "\u001b[32mINFO 2021-11-16 16:22:05 : \u001b[0mEarly stopping patience decrease to 45\n",
      "2021-11-16 16:22:09 : Epoch 39, valid result = {Average: 1.603}\n",
      "\u001b[32mINFO 2021-11-16 16:22:09 : \u001b[0mEarly stopping patience decrease to 44\n",
      "2021-11-16 16:22:13 : Epoch 40, valid result = {Average: 1.371}\n",
      "\u001b[32mINFO 2021-11-16 16:22:13 : \u001b[0mPrevious checkpoint 1637108517.875284.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:22:16 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108533.8511329.pt\n",
      "2021-11-16 16:22:20 : Epoch 41, valid result = {Average: 1.446}\n",
      "\u001b[32mINFO 2021-11-16 16:22:20 : \u001b[0mEarly stopping patience decrease to 43\n",
      "2021-11-16 16:22:23 : Epoch 42, valid result = {Average: 0.771}\n",
      "\u001b[32mINFO 2021-11-16 16:22:24 : \u001b[0mPrevious checkpoint 1637108533.8511329.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:22:27 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108544.04418.pt\n",
      "2021-11-16 16:22:31 : Epoch 43, valid result = {Average: 0.695}\n",
      "\u001b[32mINFO 2021-11-16 16:22:31 : \u001b[0mPrevious checkpoint 1637108544.04418.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:22:33 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108551.425228.pt\n",
      "2021-11-16 16:22:37 : Epoch 44, valid result = {Average: 0.777}\n",
      "\u001b[32mINFO 2021-11-16 16:22:37 : \u001b[0mEarly stopping patience decrease to 42\n",
      "2021-11-16 16:22:41 : Epoch 45, valid result = {Average: 1.928}\n",
      "\u001b[32mINFO 2021-11-16 16:22:41 : \u001b[0mEarly stopping patience decrease to 41\n",
      "2021-11-16 16:22:45 : Epoch 46, valid result = {Average: 0.640}\n",
      "\u001b[32mINFO 2021-11-16 16:22:45 : \u001b[0mPrevious checkpoint 1637108551.425228.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:22:47 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108565.200255.pt\n",
      "2021-11-16 16:22:51 : Epoch 47, valid result = {Average: 0.770}\n",
      "\u001b[32mINFO 2021-11-16 16:22:51 : \u001b[0mEarly stopping patience decrease to 40\n",
      "2021-11-16 16:22:55 : Epoch 48, valid result = {Average: 0.837}\n",
      "\u001b[32mINFO 2021-11-16 16:22:55 : \u001b[0mEarly stopping patience decrease to 39\n",
      "2021-11-16 16:22:58 : Epoch 49, valid result = {Average: 0.905}\n",
      "\u001b[32mINFO 2021-11-16 16:22:58 : \u001b[0mEarly stopping patience decrease to 38\n",
      "2021-11-16 16:23:02 : Epoch 50, valid result = {Average: 0.479}\n",
      "\u001b[32mINFO 2021-11-16 16:23:02 : \u001b[0mPrevious checkpoint 1637108565.200255.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:23:05 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108582.828937.pt\n",
      "2021-11-16 16:23:09 : Epoch 51, valid result = {Average: 0.213}\n",
      "\u001b[32mINFO 2021-11-16 16:23:09 : \u001b[0mPrevious checkpoint 1637108582.828937.pt removed due to `max_to_keep`(=1) limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO 2021-11-16 16:23:11 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108589.292416.pt\n",
      "2021-11-16 16:23:15 : Epoch 52, valid result = {Average: 0.099}\n",
      "\u001b[32mINFO 2021-11-16 16:23:15 : \u001b[0mPrevious checkpoint 1637108589.292416.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:23:17 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108595.282659.pt\n",
      "2021-11-16 16:23:21 : Epoch 53, valid result = {Average: 0.251}\n",
      "\u001b[32mINFO 2021-11-16 16:23:21 : \u001b[0mEarly stopping patience decrease to 37\n",
      "2021-11-16 16:23:25 : Epoch 54, valid result = {Average: 0.155}\n",
      "\u001b[32mINFO 2021-11-16 16:23:25 : \u001b[0mEarly stopping patience decrease to 36\n",
      "2021-11-16 16:23:28 : Epoch 55, valid result = {Average: 0.127}\n",
      "\u001b[32mINFO 2021-11-16 16:23:28 : \u001b[0mEarly stopping patience decrease to 35\n",
      "2021-11-16 16:23:32 : Epoch 56, valid result = {Average: 2.236}\n",
      "\u001b[32mINFO 2021-11-16 16:23:32 : \u001b[0mEarly stopping patience decrease to 34\n",
      "2021-11-16 16:23:37 : Epoch 57, valid result = {Average: 0.825}\n",
      "\u001b[32mINFO 2021-11-16 16:23:37 : \u001b[0mEarly stopping patience decrease to 33\n",
      "2021-11-16 16:23:41 : Epoch 58, valid result = {Average: 2.539}\n",
      "\u001b[32mINFO 2021-11-16 16:23:41 : \u001b[0mEarly stopping patience decrease to 32\n",
      "2021-11-16 16:23:45 : Epoch 59, valid result = {Average: 1.793}\n",
      "\u001b[32mINFO 2021-11-16 16:23:45 : \u001b[0mEarly stopping patience decrease to 31\n",
      "2021-11-16 16:23:49 : Epoch 60, valid result = {Average: 0.278}\n",
      "\u001b[32mINFO 2021-11-16 16:23:49 : \u001b[0mEarly stopping patience decrease to 30\n",
      "2021-11-16 16:23:53 : Epoch 61, valid result = {Average: 2.706}\n",
      "\u001b[32mINFO 2021-11-16 16:23:53 : \u001b[0mEarly stopping patience decrease to 29\n",
      "2021-11-16 16:23:57 : Epoch 62, valid result = {Average: 3.065}\n",
      "\u001b[32mINFO 2021-11-16 16:23:57 : \u001b[0mEarly stopping patience decrease to 28\n",
      "2021-11-16 16:24:01 : Epoch 63, valid result = {Average: 0.385}\n",
      "\u001b[32mINFO 2021-11-16 16:24:01 : \u001b[0mEarly stopping patience decrease to 27\n",
      "2021-11-16 16:24:06 : Epoch 64, valid result = {Average: 0.232}\n",
      "\u001b[32mINFO 2021-11-16 16:24:06 : \u001b[0mEarly stopping patience decrease to 26\n",
      "2021-11-16 16:24:10 : Epoch 65, valid result = {Average: 0.371}\n",
      "\u001b[32mINFO 2021-11-16 16:24:10 : \u001b[0mEarly stopping patience decrease to 25\n",
      "2021-11-16 16:24:16 : Epoch 66, valid result = {Average: 0.344}\n",
      "\u001b[32mINFO 2021-11-16 16:24:16 : \u001b[0mEarly stopping patience decrease to 24\n",
      "2021-11-16 16:24:20 : Epoch 67, valid result = {Average: 0.062}\n",
      "\u001b[32mINFO 2021-11-16 16:24:20 : \u001b[0mPrevious checkpoint 1637108595.282659.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:24:22 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108660.090843.pt\n",
      "2021-11-16 16:24:27 : Epoch 68, valid result = {Average: 0.627}\n",
      "\u001b[32mINFO 2021-11-16 16:24:27 : \u001b[0mEarly stopping patience decrease to 23\n",
      "2021-11-16 16:24:31 : Epoch 69, valid result = {Average: 0.115}\n",
      "\u001b[32mINFO 2021-11-16 16:24:31 : \u001b[0mEarly stopping patience decrease to 22\n",
      "2021-11-16 16:24:36 : Epoch 70, valid result = {Average: 0.056}\n",
      "\u001b[32mINFO 2021-11-16 16:24:36 : \u001b[0mPrevious checkpoint 1637108660.090843.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:24:38 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108676.1888618.pt\n",
      "2021-11-16 16:24:43 : Epoch 71, valid result = {Average: 0.052}\n",
      "\u001b[32mINFO 2021-11-16 16:24:43 : \u001b[0mPrevious checkpoint 1637108676.1888618.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:24:47 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108683.7158358.pt\n",
      "2021-11-16 16:24:52 : Epoch 72, valid result = {Average: 0.021}\n",
      "\u001b[32mINFO 2021-11-16 16:24:52 : \u001b[0mPrevious checkpoint 1637108683.7158358.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:24:55 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108692.416394.pt\n",
      "2021-11-16 16:25:00 : Epoch 73, valid result = {Average: 0.045}\n",
      "\u001b[32mINFO 2021-11-16 16:25:00 : \u001b[0mEarly stopping patience decrease to 21\n",
      "2021-11-16 16:25:04 : Epoch 74, valid result = {Average: 0.068}\n",
      "\u001b[32mINFO 2021-11-16 16:25:04 : \u001b[0mEarly stopping patience decrease to 20\n",
      "2021-11-16 16:25:08 : Epoch 75, valid result = {Average: 1.158}\n",
      "\u001b[32mINFO 2021-11-16 16:25:08 : \u001b[0mEarly stopping patience decrease to 19\n",
      "2021-11-16 16:25:13 : Epoch 76, valid result = {Average: 0.178}\n",
      "\u001b[32mINFO 2021-11-16 16:25:13 : \u001b[0mEarly stopping patience decrease to 18\n",
      "2021-11-16 16:25:17 : Epoch 77, valid result = {Average: 0.768}\n",
      "\u001b[32mINFO 2021-11-16 16:25:17 : \u001b[0mEarly stopping patience decrease to 17\n",
      "2021-11-16 16:25:21 : Epoch 78, valid result = {Average: 0.184}\n",
      "\u001b[32mINFO 2021-11-16 16:25:21 : \u001b[0mEarly stopping patience decrease to 16\n",
      "2021-11-16 16:25:25 : Epoch 79, valid result = {Average: 0.087}\n",
      "\u001b[32mINFO 2021-11-16 16:25:25 : \u001b[0mEarly stopping patience decrease to 15\n",
      "2021-11-16 16:25:29 : Epoch 80, valid result = {Average: 0.046}\n",
      "\u001b[32mINFO 2021-11-16 16:25:29 : \u001b[0mEarly stopping patience decrease to 14\n",
      "2021-11-16 16:25:33 : Epoch 81, valid result = {Average: 0.135}\n",
      "\u001b[32mINFO 2021-11-16 16:25:33 : \u001b[0mEarly stopping patience decrease to 13\n",
      "2021-11-16 16:25:37 : Epoch 82, valid result = {Average: 0.027}\n",
      "\u001b[32mINFO 2021-11-16 16:25:37 : \u001b[0mEarly stopping patience decrease to 12\n",
      "2021-11-16 16:25:41 : Epoch 83, valid result = {Average: 0.049}\n",
      "\u001b[32mINFO 2021-11-16 16:25:41 : \u001b[0mEarly stopping patience decrease to 11\n",
      "2021-11-16 16:25:45 : Epoch 84, valid result = {Average: 0.092}\n",
      "\u001b[32mINFO 2021-11-16 16:25:45 : \u001b[0mEarly stopping patience decrease to 10\n",
      "2021-11-16 16:25:49 : Epoch 85, valid result = {Average: 0.437}\n",
      "\u001b[32mINFO 2021-11-16 16:25:49 : \u001b[0mEarly stopping patience decrease to 9\n",
      "2021-11-16 16:25:53 : Epoch 86, valid result = {Average: 0.599}\n",
      "\u001b[32mINFO 2021-11-16 16:25:53 : \u001b[0mEarly stopping patience decrease to 8\n",
      "2021-11-16 16:25:57 : Epoch 87, valid result = {Average: 0.028}\n",
      "\u001b[32mINFO 2021-11-16 16:25:57 : \u001b[0mEarly stopping patience decrease to 7\n",
      "2021-11-16 16:26:01 : Epoch 88, valid result = {Average: 0.164}\n",
      "\u001b[32mINFO 2021-11-16 16:26:01 : \u001b[0mEarly stopping patience decrease to 6\n",
      "2021-11-16 16:26:05 : Epoch 89, valid result = {Average: 1.316}\n",
      "\u001b[32mINFO 2021-11-16 16:26:05 : \u001b[0mEarly stopping patience decrease to 5\n",
      "2021-11-16 16:26:09 : Epoch 90, valid result = {Average: 0.067}\n",
      "\u001b[32mINFO 2021-11-16 16:26:09 : \u001b[0mEarly stopping patience decrease to 4\n",
      "2021-11-16 16:26:13 : Epoch 91, valid result = {Average: 0.008}\n",
      "\u001b[32mINFO 2021-11-16 16:26:13 : \u001b[0mPrevious checkpoint 1637108692.416394.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 16:26:16 : \u001b[0mCurrent checkpoint saved to infilling_test_model/1637108773.7886982.pt\n",
      "2021-11-16 16:26:21 : Epoch 92, valid result = {Average: 0.033}\n",
      "\u001b[32mINFO 2021-11-16 16:26:21 : \u001b[0mEarly stopping patience decrease to 3\n",
      "2021-11-16 16:26:26 : Epoch 93, valid result = {Average: 0.032}\n",
      "\u001b[32mINFO 2021-11-16 16:26:26 : \u001b[0mEarly stopping patience decrease to 2\n",
      "2021-11-16 16:26:30 : Epoch 94, valid result = {Average: 0.013}\n",
      "\u001b[32mINFO 2021-11-16 16:26:30 : \u001b[0mEarly stopping patience decrease to 1\n",
      "2021-11-16 16:26:34 : Epoch 95, valid result = {Average: 0.043}\n",
      "\u001b[32mINFO 2021-11-16 16:26:34 : \u001b[0mEarly stopping patience exhausted, terminating training...\n",
      "\u001b[32mINFO 2021-11-16 16:26:34 : \u001b[0mTraining terminated\n"
     ]
    }
   ],
   "source": [
    "'''Resume training'''\n",
    "\n",
    "!python train_script.py\\\n",
    "    -train_src_file {train_src_file}\\\n",
    "    -train_tgt_file {train_tgt_file}\\\n",
    "    -eval_src_file {train_src_file}\\\n",
    "    -eval_tgt_file {train_tgt_file}\\\n",
    "    -config_file {model_config_file}\\\n",
    "    -save_dir {save_model_dir}\\\n",
    "    -load_from_dir {save_model_dir}\\\n",
    "    -patience 50\\\n",
    "    -valid_epoch_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_dir': 'infilling_test_model', 'gen_texts_file': 'test/toy_data/infilling/train.gen', 'src_texts_file': 'test/toy_data/infilling/train.src', 'max_decoding_length': 100, 'n_gen_per_src': 1, 'batch_size': 64, 'min_postproc_length': None, 'max_postproc_length': None, 'max_redundancy_rate': None, 'block_repeat': False, 'block_quotes': False, 'block_profanity': False, 'require_paired_punct': False, 'require_eos_punct': False, 'require_src_in_gen': False, 'force_src_in_regen': False, 'max_gen_attempts': 1, 'fallback_to_src': False, 'infer_method': 'sample', 'sample_top_k': 0, 'sample_p': 0.7, 'sample_temperature': 1.0, 'verbose': True}\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.construct:Loaded LM model from infilling_test_model/1637108773.7886982.pt\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.generate:Starting generation round 1, attempt 1...\n",
      "INFO:texgen.generate:round 1, attempt 1: generated 7 texts (0 failed requirements)...\n",
      "INFO:texgen.generate:Saved generated texts to test/toy_data/infilling/train.gen\n"
     ]
    }
   ],
   "source": [
    "'''Generate texts for source inputs'''\n",
    "\n",
    "!python generation_script.py\\\n",
    "    -src_texts_file {train_src_file}\\\n",
    "    -model_dir {save_model_dir}\\\n",
    "    -gen_texts_file {gen_output_file}\\\n",
    "    -infer_method sample\\\n",
    "    -sample_p 0.7\\\n",
    "    -verbose\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:         Harry Potter Philosopher's Stone\n",
      "GENERATED:      Harry Potter and the Philosopher's Stone\n",
      "\n",
      "SOURCE:         Harry Potter and Secrets\n",
      "GENERATED:      Harry Potter and the Chamber of Secrets\n",
      "\n",
      "SOURCE:         Harry the Prisoner of Azkaban\n",
      "GENERATED:      Harry Potter and the Prisoner of Azkaban\n",
      "\n",
      "SOURCE:         Potter and the Goblet of\n",
      "GENERATED:      Harry Potter and the Goblet of Fire\n",
      "\n",
      "SOURCE:         Harry Potter the Order the Phoenix\n",
      "GENERATED:      Harry Potter and the Order of the Phoenix\n",
      "\n",
      "SOURCE:         and the Half-Blood Prince\n",
      "GENERATED:      Harry Potter and the Half-Blood Prince\n",
      "\n",
      "SOURCE:         Harry Hallows\n",
      "GENERATED:      Harry Potter and the Deathly Hallows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''View generated texts'''\n",
    "\n",
    "with open(train_src_file) as src_f,\\\n",
    "    open(gen_output_file) as gen_f:\n",
    "    src_texts = [text.strip() for text in src_f]\n",
    "    gen_texts = [text.strip() for text in gen_f]\n",
    "\n",
    "for src, gen in zip(src_texts, gen_texts):\n",
    "    print(\"{:15s} {}\".format(\"SOURCE:\", src))\n",
    "    print(\"{:15s} {}\\n\".format(\"GENERATED:\", gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For fun: demo infilling model already trained on 10K fiction books\n",
    "\n",
    "You can download the model loaded below [here](https://drive.google.com/file/d/18E8IT__33bU24Nqws-9amY_obHZ0jVNG/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_dir': '../Documents/insentivize_trained_models/bookcorpus_10K_rand_drop/', 'gen_texts_file': 'test/toy_data/infilling/train.gen', 'src_texts_file': 'test/toy_data/infilling/train.src', 'max_decoding_length': 100, 'n_gen_per_src': 1, 'batch_size': 64, 'min_postproc_length': None, 'max_postproc_length': None, 'max_redundancy_rate': None, 'block_repeat': False, 'block_quotes': False, 'block_profanity': False, 'require_paired_punct': False, 'require_eos_punct': False, 'require_src_in_gen': True, 'force_src_in_regen': False, 'max_gen_attempts': 5, 'fallback_to_src': False, 'infer_method': 'sample', 'sample_top_k': 0, 'sample_p': 0.7, 'sample_temperature': 1.0, 'verbose': True}\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.construct:Loaded LM model from ../Documents/insentivize_trained_models/bookcorpus_10K_rand_drop/1607536509.0371842.pt\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.generate:Starting generation round 1, attempt 1...\n",
      "INFO:texgen.generate:Failed with missing source token (required tokens = ['Harry', 'Potter', 'Philosopher', \"'s\", 'Stone']): He has a college degree in the Potter Philosopher's Stone.\n",
      "INFO:texgen.generate:round 1, attempt 1: generated 7 texts (1 failed requirements)...\n",
      "INFO:texgen.generate:Starting generation round 1, attempt 2...\n",
      "INFO:texgen.generate:round 1, attempt 2: generated 1 texts (0 failed requirements)...\n",
      "INFO:texgen.generate:Saved generated texts to test/toy_data/infilling/train.gen\n"
     ]
    }
   ],
   "source": [
    "'''Generate texts for source inputs'''\n",
    "\n",
    "model_dir = \"../Documents/insentivize_trained_models/bookcorpus_10K_rand_drop/\"\n",
    "\n",
    "!python generation_script.py\\\n",
    "    -src_texts_file {train_src_file}\\\n",
    "    -model_dir {model_dir}\\\n",
    "    -gen_texts_file {gen_output_file}\\\n",
    "    -infer_method sample\\\n",
    "    -sample_p 0.7\\\n",
    "    -max_gen_attempts 5\\\n",
    "    -require_src_in_gen\\\n",
    "    -verbose\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:         Harry Potter Philosopher's Stone\n",
      "GENERATED:      The Adventures of Harry Potter and the Philosopher's Stone\n",
      "\n",
      "SOURCE:         Harry Potter and Secrets\n",
      "GENERATED:      There were some that said 'Harry Potter and Secrets'.\n",
      "\n",
      "SOURCE:         Harry the Prisoner of Azkaban\n",
      "GENERATED:      Harry remembered that the Prisoner was now in the care of Azkaban.\n",
      "\n",
      "SOURCE:         Potter and the Goblet of\n",
      "GENERATED:      \"Potter's gory: true and true, in that it is true that the Goblet of the Dead\n",
      "\n",
      "SOURCE:         Harry Potter the Order the Phoenix\n",
      "GENERATED:      The first thing I saw was the back of the door of the residence of Harry Potter and the Order of the Phoenix.\n",
      "\n",
      "SOURCE:         and the Half-Blood Prince\n",
      "GENERATED:      She was holding out the green and white stripes, which were also the real face of the half-blood prince.\n",
      "\n",
      "SOURCE:         Harry Hallows\n",
      "GENERATED:      Harry I. Glenn Hallows: In his \"life of innovation\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''View generated texts'''\n",
    "\n",
    "with open(train_src_file) as src_f,\\\n",
    "    open(gen_output_file) as gen_f:\n",
    "    src_texts = [text.strip() for text in src_f]\n",
    "    gen_texts = [text.strip() for text in gen_f]\n",
    "\n",
    "for src, gen in zip(src_texts, gen_texts):\n",
    "    print(\"{:15s} {}\".format(\"SOURCE:\", src))\n",
    "    print(\"{:15s} {}\\n\".format(\"GENERATED:\", gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion toy data\n",
    "train_src_file = \"test/toy_data/completion/train.src\"\n",
    "train_tgt_file = \"test/toy_data/completion/train.tgt\"\n",
    "model_config_file = \"test/test_configs/gpt2_lm_config.json\"\n",
    "save_model_dir = \"completion_test_model\"\n",
    "gen_output_file = \"test/toy_data/completion/train.gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:         Harry Potter and the Philosopher's\n",
      "TARGET:         Stone\n",
      "\n",
      "SOURCE:         Harry Potter and the Chamber\n",
      "TARGET:         of Secrets\n",
      "\n",
      "SOURCE:         Harry Potter and the Prisoner\n",
      "TARGET:         of Azkaban\n",
      "\n",
      "SOURCE:         Harry Potter and the Goblet of\n",
      "TARGET:         Fire\n",
      "\n",
      "SOURCE:         Harry\n",
      "TARGET:         Potter and the Order of the Phoenix\n",
      "\n",
      "SOURCE:         Harry Potter and the Half-Blood\n",
      "TARGET:         Prince\n",
      "\n",
      "SOURCE:         Harry Potter and the\n",
      "TARGET:         Deathly Hallows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Show format of source-target pairs used for training'''\n",
    "\n",
    "with open(train_src_file) as src_f,\\\n",
    "    open(train_tgt_file) as tgt_f:\n",
    "    src_texts = [text.strip() for text in src_f]\n",
    "    tgt_texts = [text.strip() for text in tgt_f]\n",
    "\n",
    "for src, tgt in zip(src_texts, tgt_texts):\n",
    "    print(\"{:15s} {}\".format(\"SOURCE:\", src))\n",
    "    print(\"{:15s} {}\\n\".format(\"TARGET:\", tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_dir': 'completion_test_model', 'config_file': 'test/test_configs/gpt2_lm_config.json', 'train_src_file': 'test/toy_data/completion/train.src', 'train_tgt_file': 'test/toy_data/completion/train.tgt', 'eval_src_file': 'test/toy_data/completion/train.src', 'eval_tgt_file': 'test/toy_data/completion/train.tgt', 'train_ref_file': None, 'eval_ref_file': None, 'max_src_length': 25, 'max_tgt_length': 75, 'load_from_dir': 'completion_test_model', 'pg_metrics': [], 'eval_metrics': [], 'batch_size': 32, 'max_epochs': 50, 'learning_rate': 0.001, 'patience': 50, 'dynamic_lr': False, 'warmup_steps': 4000, 'max_grad_norm': 5.0, 'accum_steps': 1, 'log_iterations': 100, 'valid_iterations': 1000, 'valid_epoch_end': True}\n",
      "INFO:texgen.train:Loading model configuration from completion_test_model. All hyperparameter settings will be read from here and will override any settings provided as command-line arguments.\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.construct:Saved model configuration to completion_test_model/hparams.json\n",
      "INFO:texgen.construct:Created LM model\n",
      "INFO:texgen.construct:# trainable parameters in model: 152806656\n",
      "INFO:texgen.train:Initialized model from checkpoint completion_test_model/1637106575.127691.pt\n",
      "INFO:texgen.train:Validation result prior to training:\n",
      "2021-11-16 15:50:40 : Epoch 0, test result = {Average: 0.363}\n",
      "\u001b[31mWARNING 2021-11-16 15:50:40 : Specified checkpoint directory 'completion_test_model' exists, previous checkpoints might be erased\u001b[0m\n",
      "\u001b[32mINFO 2021-11-16 15:50:40 : \u001b[0mTraining started\n",
      "\u001b[32mINFO 2021-11-16 15:50:40 : \u001b[0mModel architecture:\n",
      "LM(\n",
      "  (decoder): TransformerLMWrapper(\n",
      "    (word_embedder): WordEmbedder(\n",
      "      vocab_size=50257, embedding_dim=768\n",
      "      (_dropout_layer): EmbeddingDropout()\n",
      "    )\n",
      "    (position_embedder): PositionEmbedder(\n",
      "      position_size=1024, embedding_dim=768\n",
      "      (_dropout_layer): EmbeddingDropout()\n",
      "    )\n",
      "    (decoder): TransformerLM(\n",
      "      (_output_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      (self_attns): ModuleList(\n",
      "        (ids 0-11): MultiheadAttentionEncoder(\n",
      "          (Q_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (K_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (V_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (O_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (self_attn_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (enc_dec_attns): ModuleList(\n",
      "        (ids 0-11): MultiheadAttentionEncoder(\n",
      "          (Q_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (K_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (V_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (O_dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (end_dec_attn_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (poswise_networks): ModuleList(\n",
      "        (ids 0-11): FeedForwardNetwork(\n",
      "          (_layers): ModuleList(\n",
      "            (id 0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (id 1): GPTGELU()\n",
      "            (id 2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (poswise_layer_norm): ModuleList(\n",
      "        (ids 0-11): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (residual_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2021-11-16 15:50:44 : Epoch 1, valid result = {Average: 0.674}\n",
      "\u001b[32mINFO 2021-11-16 15:50:44 : \u001b[0mPrevious checkpoint 1637106575.127691.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:50:46 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106644.8427572.pt\n",
      "2021-11-16 15:50:51 : Epoch 2, valid result = {Average: 1.406}\n",
      "\u001b[32mINFO 2021-11-16 15:50:51 : \u001b[0mEarly stopping patience decrease to 49\n",
      "2021-11-16 15:50:55 : Epoch 3, valid result = {Average: 0.676}\n",
      "\u001b[32mINFO 2021-11-16 15:50:55 : \u001b[0mEarly stopping patience decrease to 48\n",
      "2021-11-16 15:50:59 : Epoch 4, valid result = {Average: 0.785}\n",
      "\u001b[32mINFO 2021-11-16 15:50:59 : \u001b[0mEarly stopping patience decrease to 47\n",
      "2021-11-16 15:51:03 : Epoch 5, valid result = {Average: 1.251}\n",
      "\u001b[32mINFO 2021-11-16 15:51:03 : \u001b[0mEarly stopping patience decrease to 46\n",
      "2021-11-16 15:51:07 : Epoch 6, valid result = {Average: 0.733}\n",
      "\u001b[32mINFO 2021-11-16 15:51:07 : \u001b[0mEarly stopping patience decrease to 45\n",
      "2021-11-16 15:51:11 : Epoch 7, valid result = {Average: 0.611}\n",
      "\u001b[32mINFO 2021-11-16 15:51:11 : \u001b[0mPrevious checkpoint 1637106644.8427572.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:51:13 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106671.7269251.pt\n",
      "2021-11-16 15:51:17 : Epoch 8, valid result = {Average: 0.451}\n",
      "\u001b[32mINFO 2021-11-16 15:51:17 : \u001b[0mPrevious checkpoint 1637106671.7269251.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:51:20 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106677.823821.pt\n",
      "2021-11-16 15:51:24 : Epoch 9, valid result = {Average: 0.665}\n",
      "\u001b[32mINFO 2021-11-16 15:51:24 : \u001b[0mEarly stopping patience decrease to 44\n",
      "2021-11-16 15:51:28 : Epoch 10, valid result = {Average: 0.344}\n",
      "\u001b[32mINFO 2021-11-16 15:51:28 : \u001b[0mPrevious checkpoint 1637106677.823821.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:51:30 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106688.171586.pt\n",
      "2021-11-16 15:51:34 : Epoch 11, valid result = {Average: 2.115}\n",
      "\u001b[32mINFO 2021-11-16 15:51:34 : \u001b[0mEarly stopping patience decrease to 43\n",
      "2021-11-16 15:51:38 : Epoch 12, valid result = {Average: 6.933}\n",
      "\u001b[32mINFO 2021-11-16 15:51:38 : \u001b[0mEarly stopping patience decrease to 42\n",
      "2021-11-16 15:51:42 : Epoch 13, valid result = {Average: 4.251}\n",
      "\u001b[32mINFO 2021-11-16 15:51:42 : \u001b[0mEarly stopping patience decrease to 41\n",
      "2021-11-16 15:51:46 : Epoch 14, valid result = {Average: 4.420}\n",
      "\u001b[32mINFO 2021-11-16 15:51:46 : \u001b[0mEarly stopping patience decrease to 40\n",
      "2021-11-16 15:51:50 : Epoch 15, valid result = {Average: 4.178}\n",
      "\u001b[32mINFO 2021-11-16 15:51:50 : \u001b[0mEarly stopping patience decrease to 39\n",
      "2021-11-16 15:51:54 : Epoch 16, valid result = {Average: 3.617}\n",
      "\u001b[32mINFO 2021-11-16 15:51:54 : \u001b[0mEarly stopping patience decrease to 38\n",
      "2021-11-16 15:51:59 : Epoch 17, valid result = {Average: 2.534}\n",
      "\u001b[32mINFO 2021-11-16 15:51:59 : \u001b[0mEarly stopping patience decrease to 37\n",
      "2021-11-16 15:52:03 : Epoch 18, valid result = {Average: 0.948}\n",
      "\u001b[32mINFO 2021-11-16 15:52:03 : \u001b[0mEarly stopping patience decrease to 36\n",
      "2021-11-16 15:52:07 : Epoch 19, valid result = {Average: 0.888}\n",
      "\u001b[32mINFO 2021-11-16 15:52:07 : \u001b[0mEarly stopping patience decrease to 35\n",
      "2021-11-16 15:52:11 : Epoch 20, valid result = {Average: 4.721}\n",
      "\u001b[32mINFO 2021-11-16 15:52:11 : \u001b[0mEarly stopping patience decrease to 34\n",
      "2021-11-16 15:52:15 : Epoch 21, valid result = {Average: 3.308}\n",
      "\u001b[32mINFO 2021-11-16 15:52:15 : \u001b[0mEarly stopping patience decrease to 33\n",
      "2021-11-16 15:52:19 : Epoch 22, valid result = {Average: 3.874}\n",
      "\u001b[32mINFO 2021-11-16 15:52:19 : \u001b[0mEarly stopping patience decrease to 32\n",
      "2021-11-16 15:52:23 : Epoch 23, valid result = {Average: 2.289}\n",
      "\u001b[32mINFO 2021-11-16 15:52:23 : \u001b[0mEarly stopping patience decrease to 31\n",
      "2021-11-16 15:52:27 : Epoch 24, valid result = {Average: 1.857}\n",
      "\u001b[32mINFO 2021-11-16 15:52:27 : \u001b[0mEarly stopping patience decrease to 30\n",
      "2021-11-16 15:52:31 : Epoch 25, valid result = {Average: 1.474}\n",
      "\u001b[32mINFO 2021-11-16 15:52:31 : \u001b[0mEarly stopping patience decrease to 29\n",
      "2021-11-16 15:52:35 : Epoch 26, valid result = {Average: 1.291}\n",
      "\u001b[32mINFO 2021-11-16 15:52:35 : \u001b[0mEarly stopping patience decrease to 28\n",
      "2021-11-16 15:52:39 : Epoch 27, valid result = {Average: 0.812}\n",
      "\u001b[32mINFO 2021-11-16 15:52:39 : \u001b[0mEarly stopping patience decrease to 27\n",
      "2021-11-16 15:52:43 : Epoch 28, valid result = {Average: 0.799}\n",
      "\u001b[32mINFO 2021-11-16 15:52:43 : \u001b[0mEarly stopping patience decrease to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-16 15:52:47 : Epoch 29, valid result = {Average: 0.568}\n",
      "\u001b[32mINFO 2021-11-16 15:52:47 : \u001b[0mEarly stopping patience decrease to 25\n",
      "2021-11-16 15:52:51 : Epoch 30, valid result = {Average: 0.706}\n",
      "\u001b[32mINFO 2021-11-16 15:52:51 : \u001b[0mEarly stopping patience decrease to 24\n",
      "2021-11-16 15:52:55 : Epoch 31, valid result = {Average: 0.854}\n",
      "\u001b[32mINFO 2021-11-16 15:52:55 : \u001b[0mEarly stopping patience decrease to 23\n",
      "2021-11-16 15:52:59 : Epoch 32, valid result = {Average: 0.624}\n",
      "\u001b[32mINFO 2021-11-16 15:52:59 : \u001b[0mEarly stopping patience decrease to 22\n",
      "2021-11-16 15:53:03 : Epoch 33, valid result = {Average: 0.429}\n",
      "\u001b[32mINFO 2021-11-16 15:53:03 : \u001b[0mEarly stopping patience decrease to 21\n",
      "2021-11-16 15:53:07 : Epoch 34, valid result = {Average: 0.172}\n",
      "\u001b[32mINFO 2021-11-16 15:53:07 : \u001b[0mPrevious checkpoint 1637106688.171586.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:53:09 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106787.693065.pt\n",
      "2021-11-16 15:53:13 : Epoch 35, valid result = {Average: 0.094}\n",
      "\u001b[32mINFO 2021-11-16 15:53:13 : \u001b[0mPrevious checkpoint 1637106787.693065.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:53:16 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106793.949427.pt\n",
      "2021-11-16 15:53:20 : Epoch 36, valid result = {Average: 0.767}\n",
      "\u001b[32mINFO 2021-11-16 15:53:20 : \u001b[0mEarly stopping patience decrease to 20\n",
      "2021-11-16 15:53:24 : Epoch 37, valid result = {Average: 0.074}\n",
      "\u001b[32mINFO 2021-11-16 15:53:24 : \u001b[0mPrevious checkpoint 1637106793.949427.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:53:27 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106804.727791.pt\n",
      "2021-11-16 15:53:31 : Epoch 38, valid result = {Average: 0.704}\n",
      "\u001b[32mINFO 2021-11-16 15:53:31 : \u001b[0mEarly stopping patience decrease to 19\n",
      "2021-11-16 15:53:35 : Epoch 39, valid result = {Average: 0.169}\n",
      "\u001b[32mINFO 2021-11-16 15:53:35 : \u001b[0mEarly stopping patience decrease to 18\n",
      "2021-11-16 15:53:39 : Epoch 40, valid result = {Average: 0.037}\n",
      "\u001b[32mINFO 2021-11-16 15:53:39 : \u001b[0mPrevious checkpoint 1637106804.727791.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:53:41 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106819.8563552.pt\n",
      "2021-11-16 15:53:46 : Epoch 41, valid result = {Average: 0.434}\n",
      "\u001b[32mINFO 2021-11-16 15:53:46 : \u001b[0mEarly stopping patience decrease to 17\n",
      "2021-11-16 15:53:50 : Epoch 42, valid result = {Average: 0.028}\n",
      "\u001b[32mINFO 2021-11-16 15:53:50 : \u001b[0mPrevious checkpoint 1637106819.8563552.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:53:53 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106830.359982.pt\n",
      "2021-11-16 15:53:57 : Epoch 43, valid result = {Average: 0.189}\n",
      "\u001b[32mINFO 2021-11-16 15:53:57 : \u001b[0mEarly stopping patience decrease to 16\n",
      "2021-11-16 15:54:01 : Epoch 44, valid result = {Average: 0.019}\n",
      "\u001b[32mINFO 2021-11-16 15:54:01 : \u001b[0mPrevious checkpoint 1637106830.359982.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:54:03 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106841.682401.pt\n",
      "2021-11-16 15:54:07 : Epoch 45, valid result = {Average: 0.563}\n",
      "\u001b[32mINFO 2021-11-16 15:54:07 : \u001b[0mEarly stopping patience decrease to 15\n",
      "2021-11-16 15:54:12 : Epoch 46, valid result = {Average: 0.026}\n",
      "\u001b[32mINFO 2021-11-16 15:54:12 : \u001b[0mEarly stopping patience decrease to 14\n",
      "2021-11-16 15:54:16 : Epoch 47, valid result = {Average: 0.189}\n",
      "\u001b[32mINFO 2021-11-16 15:54:16 : \u001b[0mEarly stopping patience decrease to 13\n",
      "2021-11-16 15:54:20 : Epoch 48, valid result = {Average: 0.024}\n",
      "\u001b[32mINFO 2021-11-16 15:54:20 : \u001b[0mEarly stopping patience decrease to 12\n",
      "2021-11-16 15:54:25 : Epoch 49, valid result = {Average: 0.009}\n",
      "\u001b[32mINFO 2021-11-16 15:54:25 : \u001b[0mPrevious checkpoint 1637106841.682401.pt removed due to `max_to_keep`(=1) limit\n",
      "\u001b[32mINFO 2021-11-16 15:54:27 : \u001b[0mCurrent checkpoint saved to completion_test_model/1637106865.263202.pt\n",
      "2021-11-16 15:54:31 : Epoch 50, valid result = {Average: 0.015}\n",
      "\u001b[32mINFO 2021-11-16 15:54:31 : \u001b[0mEarly stopping patience decrease to 11\n",
      "\u001b[32mINFO 2021-11-16 15:54:31 : \u001b[0mTraining terminated\n"
     ]
    }
   ],
   "source": [
    "'''Train a model to predict target texts from source inputs'''\n",
    "\n",
    "!python train_script.py\\\n",
    "    -train_src_file {train_src_file}\\\n",
    "    -train_tgt_file {train_tgt_file}\\\n",
    "    -eval_src_file {train_src_file}\\\n",
    "    -eval_tgt_file {train_tgt_file}\\\n",
    "    -config_file {model_config_file}\\\n",
    "    -save_dir {save_model_dir}\\\n",
    "    -patience 100\\\n",
    "    -valid_epoch_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_dir': 'completion_test_model', 'gen_texts_file': 'test/toy_data/completion/train.gen', 'src_texts_file': 'test/toy_data/completion/train.src', 'max_decoding_length': 100, 'n_gen_per_src': 1, 'batch_size': 64, 'min_postproc_length': None, 'max_postproc_length': None, 'max_redundancy_rate': None, 'block_repeat': False, 'block_quotes': False, 'block_profanity': False, 'require_paired_punct': False, 'require_eos_punct': False, 'require_src_in_gen': False, 'force_src_in_regen': False, 'max_gen_attempts': 1, 'fallback_to_src': False, 'infer_method': 'sample', 'sample_top_k': 0, 'sample_p': 0.7, 'sample_temperature': 1.0, 'verbose': True}\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.construct:Loaded LM model from completion_test_model/1637106865.263202.pt\n",
      "Using cached pre-trained GPT2 checkpoint from /Users/mroemmele/texar_data/GPT2/gpt2-small.\n",
      "INFO:texgen.generate:Starting generation round 1, attempt 1...\n",
      "INFO:texgen.generate:round 1, attempt 1: generated 7 texts (0 failed requirements)...\n",
      "INFO:texgen.generate:Saved generated texts to test/toy_data/completion/train.gen\n"
     ]
    }
   ],
   "source": [
    "'''Generate texts for source inputs'''\n",
    "\n",
    "!python generation_script.py\\\n",
    "    -src_texts_file {train_src_file}\\\n",
    "    -model_dir {save_model_dir}\\\n",
    "    -gen_texts_file {gen_output_file}\\\n",
    "    -infer_method sample\\\n",
    "    -sample_p 0.7\\\n",
    "    -verbose\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:         Harry Potter and the Philosopher's\n",
      "GENERATED:      Stone\n",
      "\n",
      "SOURCE:         Harry Potter and the Chamber\n",
      "GENERATED:      of Secrets\n",
      "\n",
      "SOURCE:         Harry Potter and the Prisoner\n",
      "GENERATED:      of Azkaban\n",
      "\n",
      "SOURCE:         Harry Potter and the Goblet of\n",
      "GENERATED:      Fire\n",
      "\n",
      "SOURCE:         Harry\n",
      "GENERATED:      Potter and the Order of the Phoenix\n",
      "\n",
      "SOURCE:         Harry Potter and the Half-Blood\n",
      "GENERATED:      Prince\n",
      "\n",
      "SOURCE:         Harry Potter and the\n",
      "GENERATED:      Deathly Hallows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''View generated texts'''\n",
    "\n",
    "with open(train_src_file) as src_f,\\\n",
    "    open(gen_output_file) as gen_f:\n",
    "    src_texts = [text.strip() for text in src_f]\n",
    "    gen_texts = [text.strip() for text in gen_f]\n",
    "\n",
    "for src, gen in zip(src_texts, gen_texts):\n",
    "    print(\"{:15s} {}\".format(\"SOURCE:\", src))\n",
    "    print(\"{:15s} {}\\n\".format(\"GENERATED:\", gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
